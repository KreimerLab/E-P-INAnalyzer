{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f221333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Manipulation/Management Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization Libaries\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Bioinformatics Libraries\n",
    "import pybedtools\n",
    "\n",
    "# Statistical Tests, Machine Learning, etc. Libraries\n",
    "import igraph as ig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Input/Output Libraries\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Miscellaneous Libraries \n",
    "import time\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4666581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlaps(df1, df2):\n",
    "    # Merge on 'Chrom' to align intervals from the same chromosome\n",
    "    df1 = df1[['Chrom', 'Start', 'End']]\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df2 = df2[['Chrom', 'Start', 'End']]\n",
    "    df2 = df2.drop_duplicates()\n",
    "    \n",
    "    merged_df = pd.merge(df1, df2, on='Chrom', suffixes=('_x', '_y'))\n",
    "    \n",
    "    # Convert 'Start' and 'End' to integers if they aren't already\n",
    "    merged_df['Start_x'] = merged_df['Start_x'].astype(int)\n",
    "    merged_df['End_x'] = merged_df['End_x'].astype(int)\n",
    "    merged_df['Start_y'] = merged_df['Start_y'].astype(int)\n",
    "    merged_df['End_y'] = merged_df['End_y'].astype(int)\n",
    "    \n",
    "    # Find intersections\n",
    "    intersections = merged_df[\n",
    "        (merged_df['Start_x'] <= merged_df['End_y']) &\n",
    "        (merged_df['End_x'] >= merged_df['Start_y'])\n",
    "    ]\n",
    "    \n",
    "    # Return the count of unique overlapping intervals\n",
    "    return intersections.drop_duplicates().shape[0]\n",
    "\n",
    "def permutation_test_for_enrichment(unfiltered_edges_path, filtered_edges_path, mpra_dataset, permutations=1000):\n",
    "    # Load the datasets\n",
    "    unfiltered_df = pd.read_csv(unfiltered_edges_path, delimiter='\\t')\n",
    "    filtered_df = pd.read_csv(filtered_edges_path, delimiter='\\t')\n",
    "    mpra_df = pd.read_csv(mpra_dataset, delimiter='\\t')  # Assuming similar structure/format\n",
    "    \n",
    "    # Calculate actual overlaps with filtered network\n",
    "    actual_overlaps = calculate_overlaps(filtered_df, mpra_df)\n",
    "    \n",
    "    # Get the number of enhancers in the filtered network\n",
    "    filtered_df = filtered_df[['Chrom', 'Start', 'End']]\n",
    "    filtered_df = filtered_df.drop_duplicates()\n",
    "    print(filtered_df)\n",
    "    number_of_filtered_enhancers = filtered_df.shape[0]\n",
    "    print(number_of_filtered_enhancers)\n",
    "    \n",
    "    # Permutation test\n",
    "    permutation_overlaps = []\n",
    "    for _ in range(permutations):\n",
    "        # Sample enhancers from the unfiltered network\n",
    "        sampled_enhancers = unfiltered_df.sample(n=number_of_filtered_enhancers, replace=False)\n",
    "        \n",
    "        # Calculate overlaps with MPRA dataset\n",
    "        overlaps = calculate_overlaps(sampled_enhancers, mpra_df)\n",
    "        permutation_overlaps.append(overlaps)\n",
    "    \n",
    "    # Calculate empirical p-value\n",
    "    p_value = np.mean(np.array(permutation_overlaps) >= actual_overlaps)\n",
    "    \n",
    "    # Calculate the average number of hits per permuted network\n",
    "    average_hits_per_permutation = np.mean(permutation_overlaps)\n",
    "    \n",
    "    return actual_overlaps, p_value, average_hits_per_permutation\n",
    "\n",
    "actual_overlaps, p_value, average_hits_per_permutation = permutation_test_for_enrichment(\n",
    "    'Network/Components/ABC_Network_Unfiltered_Edges.tsv',\n",
    "    'Network/ABC_Network_Filtered_Edges.tsv',\n",
    "    'MPRA/Processed_Pertubation_MPRA.tsv',\n",
    "    permutations=1  # Adjust the number of permutations as needed\n",
    ")\n",
    "print(f'Hits in the true (filtered) network: {actual_overlaps}')\n",
    "print(f'Empirical p-value: {p_value}')\n",
    "print(f'Average hits per permuted network: {average_hits_per_permutation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00dccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Substructures of Enhancer-Promoter Interactions\n",
    "\n",
    "def label_clusters(cluster: list):\n",
    "    if len(cluster) == 2:\n",
    "        return \"Enhancer --> Gene (1:1)\"\n",
    "    \n",
    "    elif len(cluster) > 2:\n",
    "        module_count = 0\n",
    "        gene_count = 0\n",
    "        for item in cluster:\n",
    "            if item.startswith(\"Module_\"):\n",
    "                module_count += 1\n",
    "            else:\n",
    "                gene_count += 1\n",
    "        if module_count == len(cluster) - 1:\n",
    "            return \"Enhancers --> Gene (2+:1)\" \n",
    "        elif gene_count == len(cluster) - 1:\n",
    "            return \"Enhancer --> Genes (1:2+)\"\n",
    "    \n",
    "    return \"Enhancers --> Genes (2+:2+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Approach \n",
    "\n",
    "def cluster_betweenness(file_name: str):\n",
    "    \n",
    "    df = pd.read_csv(file_name, sep = '\\t')\n",
    "    df = df.loc[:, [\"Module\", \"Gene\"]]\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    tuples = [tuple(x) for x in df.values]\n",
    "    g = ig.Graph.TupleList(tuples, directed = True)\n",
    "    node_labels = g.vs['name']\n",
    "    \n",
    "    communities = g.community_edge_betweenness().as_clustering()\n",
    "    \n",
    "    communities_dict = {}\n",
    "    \n",
    "    for i, c in enumerate(communities):\n",
    "        for node_index in c:\n",
    "            communities_dict[node_labels[node_index]] = i\n",
    "            \n",
    "    clusteres = {}\n",
    "    for key, value in communities_dict.items():\n",
    "        clusteres.setdefault(value, []).append(key)\n",
    "        \n",
    "    return clusteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5044d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enhancer_regions_bed_path = 'Network/ABC_Network_Filtered_Regions.bed'\n",
    "\n",
    "enhancer_regions_df = pd.read_csv(enhancer_regions_bed_path, sep='\\t', header=None, names=['Chrom', 'Start', 'End'])\n",
    "\n",
    "variants_bed_path = 'Stephan_QuarterMil_denovoSNPs_posAllelePheno.bed'\n",
    "variants_df = pd.read_csv(variants_bed_path, sep='\\t', header=None, names=['Chrom', 'Start', 'End'])\n",
    "\n",
    "network_file_path = 'Network/ABC_Network_Filtered_Edges.tsv'\n",
    "network_df = pd.read_csv(network_file_path, sep='\\t')\n",
    "\n",
    "def check_variant_presence(row, variants_df):\n",
    "    enhancer_variants = variants_df[(variants_df['Chrom'] == row['Chrom']) &\n",
    "                                    (variants_df['Start'] >= row['Start']) &\n",
    "                                    (variants_df['End'] <= row['End'])]\n",
    "    return not enhancer_variants.empty\n",
    "\n",
    "network_df['Has_Variant'] = network_df.apply(check_variant_presence, variants_df=variants_df, axis=1)\n",
    "\n",
    "unique_enhancers_with_variants = network_df[network_df['Has_Variant']].drop_duplicates(subset=['Chrom', 'Start', 'End'])\n",
    "print(f\"Number of unique enhancers with variants: {len(unique_enhancers_with_variants)}\")\n",
    "\n",
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_distribution_with_variants(clusters: dict, network_data_with_variants: pd.DataFrame):\n",
    "    \n",
    "    distribution = {\n",
    "        \"Enhancer --> Gene (1:1)\": {\"count\": 0, \"with_variants\": 0},\n",
    "        \"Enhancers --> Gene (2+:1)\": {\"count\": 0, \"with_variants\": 0},\n",
    "        \"Enhancer --> Genes (1:2+)\": {\"count\": 0, \"with_variants\": 0},\n",
    "        \"Enhancers --> Genes (2+:2+)\": {\"count\": 0, \"with_variants\": 0}\n",
    "    }\n",
    "    \n",
    "    for cluster_id, items in clusters.items():\n",
    "        label = label_clusters(items)\n",
    "        distribution[label][\"count\"] += 1\n",
    "        \n",
    "        has_variant = any(network_data_with_variants[network_data_with_variants['Module'].isin(items)]['Has_Variant'])\n",
    "        if has_variant:\n",
    "            distribution[label][\"with_variants\"] += 1\n",
    "    \n",
    "    for label, stats in distribution.items():\n",
    "        percent_with_variants = (stats[\"with_variants\"] / stats[\"count\"]) * 100 if stats[\"count\"] else 0\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"  Total Clusters: {stats['count']}\")\n",
    "        print(f\"  Clusters with Variants: {stats['with_variants']} ({percent_with_variants:.2f}%)\")\n",
    "\n",
    "clusters = cluster_betweenness('/home/wbd20/Miscellaneous/Kreimer_Lab/Network/ABC_Network_Filtered_Edges.tsv')\n",
    "cluster_distribution_with_variants(clusters, network_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "asd_genes_path = 'E-P-INs/Variants/ASD_Genes.tsv'\n",
    "asd_genes_df = pd.read_csv(asd_genes_path, sep='\\t')\n",
    "\n",
    "asd_genes_set = set(asd_genes_df['Gene'])\n",
    "\n",
    "network_df['Has_Gene'] = network_df['Gene'].apply(lambda gene: gene in asd_genes_set)\n",
    "\n",
    "unique_asd_genes = network_df[network_df['Has_Gene']].drop_duplicates(subset=['Chrom', 'Start', 'End'])\n",
    "print(f\"Number of unique ASD-associated genes: {len(unique_asd_genes)}\")\n",
    "\n",
    "network_df_backup = network_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = network_df.drop('Chrom', axis = 1)\n",
    "network_df = network_df.drop('Start', axis = 1)\n",
    "network_df = network_df.drop('End', axis = 1)\n",
    "network_df = network_df.drop('Sample', axis = 1)\n",
    "network_df = network_df.drop_duplicates()\n",
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "actual_true_pairings = ((network_df['Has_Variant'] == True) & (network_df['Has_Gene'] == True)).sum()\n",
    "\n",
    "n_permutations = 1000\n",
    "count_true_pairings_permutations = []\n",
    "\n",
    "for _ in range(n_permutations):\n",
    "    shuffled_has_variant = network_df['Has_Gene'].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    \n",
    "    count = ((shuffled_has_variant == True) & (network_df['Has_Variant'] == True)).sum()\n",
    "    count_true_pairings_permutations.append(count)\n",
    "\n",
    "average_hits_permutations = np.mean(count_true_pairings_permutations)\n",
    "\n",
    "p_value = np.mean([count >= actual_true_pairings for count in count_true_pairings_permutations])\n",
    "\n",
    "print(f\"Actual number of true hits: {actual_true_pairings}\")\n",
    "print(f\"Average number of hits in permutations: {average_hits_permutations}\")\n",
    "print(f\"Empirical p-value: {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
